import argparse
from configparser import ConfigParser
import json
import sys
import re
import logging
import os
import time
import random
import ast
import uuid
import threading
from threading import Thread


def config_logs():
    log_format = "%(levelname)s %(asctime)s - %(message)s"
    logging.basicConfig(filename='logs.txt',
                        level=logging.DEBUG,
                        format=log_format,
                        filemode='w')


def config_parser():
    config = ConfigParser()
    config.read('configuration.ini')

    parser = argparse.ArgumentParser(prog='Data Generator', description='Generate dataset from given schema.')

    parser.add_argument('-p', '--path_to_files', type=str, default=config['DEFAULT']['path_to_files'],
                        help='The path where files should be saved "." means current directory')

    parser.add_argument('-f', '--files_count', type=int, default=config['DEFAULT']['files_count'],
                        help='How many files should be generated. If 0, then all output will be written in console')

    parser.add_argument('-b', '--base_file_name', type=str, default=config['DEFAULT']['base_file_name'],
                        help='Base name of each file. If "files_count" > 1 base name will be continued with prefix')

    parser.add_argument('-x', '--file_prefix', default=config['DEFAULT']['file_prefix'],
                        choices=['count', 'random', 'uuid'],
                        help='Prefix placed after basename when files_count is more than 1')

    parser.add_argument('-d', '--data_schema', type=str, default=config['DEFAULT']['data_schema'],
                        help='Schema of generated data')

    parser.add_argument('-l', '--data_lines', type=int, default=config['DEFAULT']['data_lines'],
                        help='Number of lines in each JSON file')

    parser.add_argument('-c', '--clear_path', action='store_false',
                        help='Use this argument to clear all files in directory, witch match the base_file_name')

    parser.add_argument('-m', '--multiprocessing', type=int, default=config['DEFAULT']['multiprocessing'],
                        help='How many processes should be used to create files.')

    args = parser.parse_args()

    return args


def identify_schema_source(inputs):
    try:
        schema = json.loads(inputs)
        if not isinstance(schema, dict):
            schema = json.loads(schema)
    except json.decoder.JSONDecodeError:
        if os.path.exists(inputs):
            with open(f'{inputs}', 'r') as f:
                schema = json.load(f)
                if not isinstance(schema, dict):
                    schema = json.loads(schema)
        else:
            sys.exit('Path to file with JSON data schema doesnt exist')
    return schema


def process_schema(dic):
    """Check correctness of a given data schema"""
    possible_keys = ['date', 'name', 'type', 'age']
    possible_types = ['timestamp', 'int', 'str', ]
    pattern_rand_1 = '^[0-9].*'
    pattern_rand_2 = '^[a-z].*'
    pattern_rand_3 = '\[.*\]'
    pattern_rand_4 = 'rand\([0-9]*, ?[0-9]*\)'
    for key in dic:
        if key.lower() not in possible_keys:
            sys.exit(f'forbidden key in JSON input -> {key}')
        if ':' in dic[key]:
            divider = dic[key].index(':')
            left = dic[key][:divider]
            right = dic[key][divider + 1:]
            if left not in possible_types:
                sys.exit(f'forbidden type of data to generate -> {left}')
            if re.match(pattern_rand_2, right.lower()) or re.match(pattern_rand_1, right.lower()):
                pass
            elif re.match(pattern_rand_3, right.lower()):
                pass
            elif right == '':
                pass
            else:
                sys.exit(f'forbidden right side of notation -> {right}')

        else:
            sys.exit('forbidden format of input. Value must be like "type of data to generate:what to generate"')

        """Generating data line"""
        if key.lower() == 'date':
            if not dic[key].lower().startswith('timestamp'):
                sys.exit('type of date must be a timestamp')
            elif not dic[key].lower().endswith(':'):
                pass
                # logging.warning('timestamp does not support any values. Value will be replaced with current date')
            date = time.time()

        if key.lower() == 'name':
            if not dic[key].lower().startswith('str:'):
                sys.exit('Name must be a "str"')
            if dic[key][4:].lower() == 'rand':
                name = str(uuid.uuid4())
            elif re.match(pattern_rand_3, dic[key][4:]):
                name = random.choice(ast.literal_eval(dic[key][4:]))
            else:
                name = dic[key][4:]

        if key.lower() == 'type':
            if not dic[key].lower().startswith('str:'):
                sys.exit('Type must be a "str"')
            if re.match(pattern_rand_3, dic[key][4:].lower()):
                typ = random.choice(ast.literal_eval(dic[key][4:]))
            else:
                typ = dic[key][4:]

        if key.lower() == 'age':
            if not dic[key].lower().startswith('int:'):
                sys.exit('age must be an "int"')
            if re.match(pattern_rand_3, dic[key][4:].lower()):
                age = random.choice(ast.literal_eval(dic[key][4:]))
                if not isinstance(int(age), int):
                    sys.exit(f'every item in list to draw has to be an "int" -> {age}')
            elif dic[key][4:].lower() == 'rand':
                age = random.randint(0, 10000)
            elif re.match(pattern_rand_4, dic[key][4:]):
                comma = dic[key].index(',')
                p1 = dic[key].index('(')
                p2 = dic[key].index(')')
                floor = dic[key][p1 + 1:comma]
                ceiling = dic[key][comma + 1:p2]
                if not isinstance(int(floor), int) or not isinstance(int(ceiling), int):
                    sys.exit(f'given scope has to be an "int" -> {floor} or {ceiling}')
                age = random.randint(int(floor), int(ceiling))
            else:
                age = dic[key][4:]
                try:
                    age = int(age)
                except ValueError:
                    pass
                if not isinstance(age, int):
                    sys.exit(f'given age has to be an "int" -> {age}')
    return {"date": date, "name": name, "type": typ, "age": age}


def process_attributes(attrs_dict):
    if attrs_dict['path_to_files'].lower() == 'current':
        saving_path = os.getcwd()
    elif os.path.exists(attrs_dict['path_to_files']):
        saving_path = attrs_dict['path_to_files']
    elif not os.path.exists(attrs_dict['path_to_files']):
        sys.exit(f'Path {attrs_dict["path_to_files"]} does not exist')

    if attrs_dict['files_count'] < 0:
        sys.exit('I cant create negative number of files')
    elif attrs_dict['files_count'] == 0:
        how_to_display = 'console'
        set_prefix = False
    elif attrs_dict['files_count'] == 1:
        how_to_display = 'files'
        set_prefix = False
    elif attrs_dict['files_count'] > 1:
        how_to_display = 'files'
        set_prefix = attrs_dict['file_prefix']

    if attrs_dict['multiprocessing'] <= 0:
        sys.exit('I cant run negative or none number of processes')
    elif attrs_dict['multiprocessing'] == 1:
        number_of_processes = 1
    else:
        number_of_processes = min(attrs_dict['multiprocessing'], os.cpu_count())

    return [saving_path, [how_to_display, attrs_dict['files_count']], attrs_dict['base_file_name'], set_prefix,
            attrs_dict['data_lines'], attrs_dict['clear_path'], number_of_processes]


def clearing_path(path, flag, filename):
    if flag:
        for file in os.listdir(path):
            if file.startswith(filename):
                os.remove(path + '/' + file)
    else:
        return False


def generate_dataset_basic(attributes, data):
    """setting way of display"""
    if attributes[1][0] == 'console':
        for _ in range(attributes[4]):
            data_line = process_schema(data)
            print(data_line)
    else:
        if not attributes[3]:
            with open(f'{attributes[0]}/{attributes[2]}.txt', 'w', encoding='utf-8') as f:
                for _ in range(attributes[4]):
                    line = json.dumps(process_schema(data))
                    f.write(f'{line}\n')

        elif attributes[3].lower() == 'count':
            for i in range(attributes[1][1]):
                with open(f'{attributes[0]}/{attributes[2]}_{i}.txt', 'w', encoding='utf-8') as f:
                    for _ in range(attributes[4]):
                        line = json.dumps(process_schema(data))
                        f.write(f'{line}\n')

        elif attributes[3].lower() == 'uuid':
            for _ in range(attributes[1][1]):
                with open(f'{attributes[0]}/{attributes[2]}_{str(uuid.uuid4())}.txt', 'w', encoding='utf-8') as f:
                    for _ in range(attributes[4]):
                        line = json.dumps(process_schema(data))
                        f.write(f'{line}\n')
        else:
            sys.exit(f'forbidden prefix for files -> {attributes[3]}')


counter = 0
lock = threading.Lock()
def generate_dataset_parallel(attributes, data, lock):
    global counter
    while True:
        with lock:
            if counter >= attributes[1][1]:
                break
            else:
                counter += 1
        if not attributes[3]:
            sys.exit('Creating one file with few processes is no sense')
        elif attributes[3].lower() == 'count':
            with open(f'{attributes[0]}/{attributes[2]}_{counter}.txt', 'w', encoding='utf-8') as f:
                for _ in range(attributes[4]):
                    line = json.dumps(process_schema(data))
                    f.write(f'{line}\n')

        elif attributes[3].lower() == 'uuid':
            with open(f'{attributes[0]}/{attributes[2]}_{str(uuid.uuid4())}.txt', 'w', encoding='utf-8') as f:
                for _ in range(attributes[4]):
                    line = json.dumps(process_schema(data))
                    f.write(f'{line}\n')
        else:
            sys.exit(f'forbidden prefix for files -> {attributes[3]}')




if __name__ == '__main__':
    config_logs()
    args = config_parser()
    schema = identify_schema_source(args.data_schema)

    logging.debug('processing given attributes...')
    proc_at = process_attributes(vars(args))
    logging.info('attributes processed')

    logging.debug('starting clearing path...')
    clearing_path(proc_at[0], proc_at[5], proc_at[2])
    logging.info('path clear.')

    if proc_at[6] == 1:
        start = time.time()
        logging.debug('checking correctness and generating the data in a single process...')
        generate_dataset_basic(proc_at, schema)
        logging.info('data generated')
        stop = time.time()
        print(stop-start)
    else:
        start = time.time()
        if proc_at[1][0] == 'console':
            sys.exit('Printing data to console with many processes is no sense')
        logging.debug(f'checking correctness and generating the data in {proc_at[6]} processes...')
        threads = [Thread(target=generate_dataset_parallel, args=(proc_at, schema, lock)) for _ in range(proc_at[6])]
        [t.start() for t in threads]
        [t.join() for t in threads]
        logging.info('data generated')
        stop = time.time()
        print(stop-start)
